\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand*\new@tpo@label[2]{}
\catcode `"\active 
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\AC@reset@newl@bel
\citation{kramer}
\citation{corves}
\citation{rey_wender}
\select@language{ngerman}
\@writefile{toc}{\select@language{ngerman}}
\@writefile{lof}{\select@language{ngerman}}
\@writefile{lot}{\select@language{ngerman}}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Vorbild der KNN und das Prinzip eines k\"unstlichen Neurons}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lol}{\addvspace {10\p@ }}
\newlabel{cKnn}{{1}{1}{Vorbild der KNN und das Prinzip eines künstlichen Neurons}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Nat\"urliche neuronale Netze}{1}{section.1.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Aufbau einer Nervenzelle\relax }}{2}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{neuronaufbau}{{1.1}{2}{Aufbau einer Nervenzelle\relax }{figure.caption.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Funktionsweise und Aufbau eines k\"unstlichen Neurons}{2}{section.1.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Funktionsweise eines Neurons, modifizierte Version aus \url  {http://www.codeplanet.eu/tutorials/csharp/70-kuenstliche-neuronale-netze-in-csharp.html}\relax }}{3}{figure.caption.2}}
\newlabel{funktionNeuron}{{1.2}{3}{Funktionsweise eines Neurons, modifizierte Version aus \url {http://www.codeplanet.eu/tutorials/csharp/70-kuenstliche-neuronale-netze-in-csharp.html}\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}\"Ubertragungsfunktion}{3}{subsection.1.2.1}}
\newlabel{eq:net_j}{{1.1}{3}{Übertragungsfunktion}{equation.1.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Aktivierungsfunktion}{3}{subsection.1.2.2}}
\@writefile{lot}{\contentsline {table}{\numberline {1.1}{\ignorespaces lineare Aktivierungsfunktion, ReLu und die bin\"are Aktivierungsfunktion\relax }}{4}{table.caption.3}}
\newlabel{t_actFunc}{{1.1}{4}{lineare Aktivierungsfunktion, ReLu und die binäre Aktivierungsfunktion\relax }{table.caption.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1.2}{\ignorespaces Logistische und $\qopname  \relax o{tanh}$ Aktivierungsfunktion\relax }}{5}{table.caption.4}}
\newlabel{t_actFunc2}{{1.2}{5}{Logistische und $\tanh $ Aktivierungsfunktion\relax }{table.caption.4}{}}
\newlabel{eq:actFunc_Schw}{{1.2}{6}{Aktivierungsfunktion}{equation.1.2.2}{}}
\AC@undonewlabel{acro:KNN}
\newlabel{acro:KNN}{{1.2.2}{6}{Aktivierungsfunktion}{section*.5}{}}
\acronymused{KNN}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Ausgangsfunktion}{6}{subsection.1.2.3}}
\acronymused{KNN}
\newlabel{eq:fout_id}{{1.3}{6}{Ausgangsfunktion}{equation.1.2.3}{}}
\citation{scherer}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Struktur der KNN}{7}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lol}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Aufbaustruktur}{7}{section.2.1}}
\acronymused{KNN}
\acronymused{KNN}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Schematisches Beispiel Feedforward Netzwerk\relax }}{7}{figure.caption.6}}
\newlabel{abb:feedforwardNetz}{{2.1}{7}{Schematisches Beispiel Feedforward Netzwerk\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Schematisches Beispiel Feedback Netzwerk\relax }}{7}{figure.caption.6}}
\newlabel{feedbackNetz}{{2.2}{7}{Schematisches Beispiel Feedback Netzwerk\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Beispiel Feedforward Netzwerk, modifizierte Version aus \url  {http://www.texample.net/tikz/examples/neural-network/}\relax }}{8}{figure.caption.7}}
\newlabel{ffNetz_Aufbau}{{2.3}{8}{Beispiel Feedforward Netzwerk, modifizierte Version aus \url {http://www.texample.net/tikz/examples/neural-network/}\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Einlagiges Perzeptron}{9}{section.2.2}}
\acronymused{KNN}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Darstellung der booleschen Funktionen als einfaches Perzeptron}{9}{subsection.2.2.1}}
\newlabel{ch:boolNeuron}{{2.2.1}{9}{Darstellung der booleschen Funktionen als einfaches Perzeptron}{subsection.2.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Repr\"asentation der Konjunktion $x_1 \wedge x_2$ als einfaches Perzeptron\relax }}{9}{figure.caption.8}}
\newlabel{abb:AND-perceptron}{{2.4}{9}{Repräsentation der Konjunktion $x_1 \wedge x_2$ als einfaches Perzeptron\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Repr\"asentation der Disjunktion $x_1 \vee x_2$ als einfaches Perzeptron\relax }}{9}{figure.caption.8}}
\newlabel{abb:OR-perceptron}{{2.5}{9}{Repräsentation der Disjunktion $x_1 \vee x_2$ als einfaches Perzeptron\relax }{figure.caption.8}{}}
\newlabel{eq:AND-function2}{{2.2}{10}{Darstellung der booleschen Funktionen als einfaches Perzeptron}{equation.2.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Klassifizierung und lineare Separierbarkeit}{10}{subsection.2.2.2}}
\acronymused{KNN}
\newlabel{eq:AND_geradenfunc}{{2.6}{11}{Klassifizierung und lineare Separierbarkeit}{equation.2.2.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Beispiel 1 f\"ur die lineare Separierbarkeit der AND-Funktion\relax }}{11}{figure.caption.9}}
\newlabel{abb:geradengleichung_AND}{{2.6}{11}{Beispiel 1 für die lineare Separierbarkeit der AND-Funktion\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Beispiel 2 f\"ur die lineare Separierbarkeit der AND-Funktion\relax }}{11}{figure.caption.9}}
\newlabel{abb:geradengleichung_AND2}{{2.7}{11}{Beispiel 2 für die lineare Separierbarkeit der AND-Funktion\relax }{figure.caption.9}{}}
\citation{minsky_papert}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Das XOR-Problem}{12}{subsection.2.2.3}}
\acronymused{KNN}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Die OR-Funktion ist nicht linear separierbar\relax }}{12}{figure.caption.10}}
\newlabel{abb:geradengleichung_OR}{{2.8}{12}{Die OR-Funktion ist nicht linear separierbar\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Mehrlagiges Perzeptron}{12}{section.2.3}}
\AC@undonewlabel{acro:MLP}
\newlabel{acro:MLP}{{2.3}{12}{Mehrlagiges Perzeptron}{section*.11}{}}
\acronymused{MLP}
\acronymused{MLP}
\acronymused{MLP}
\acronymused{MLP}
\acronymused{KNN}
\acronymused{KNN}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Darstellung der XOR-Funktion als MLP}{13}{subsection.2.3.1}}
\acronymused{KNN}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Durch die Kombination von drei Perzeptronen, kann die XOR-Funktion gel\"ost werden.\relax }}{13}{figure.caption.12}}
\newlabel{abb:XOR-Perzeptron_ohne_EingabeNeuronen}{{2.9}{13}{Durch die Kombination von drei Perzeptronen, kann die XOR-Funktion gelöst werden.\relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Bias-Neuronen}{13}{subsection.2.3.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces \ac {MLP}, der die XOR-Funktion l\"osen kann ohne Bias-Neuronen\relax }}{14}{figure.caption.13}}
\acronymused{MLP}
\newlabel{abb:XOR-Perceptron}{{2.10}{14}{\ac {MLP}, der die XOR-Funktion lösen kann ohne Bias-Neuronen\relax }{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces \ac {MLP}, der die XOR-Funktion l\"osen kann mit Bias-Neuronen\relax }}{14}{figure.caption.14}}
\acronymused{MLP}
\newlabel{abb:XOR-Perceptron-bias}{{2.11}{14}{\ac {MLP}, der die XOR-Funktion lösen kann mit Bias-Neuronen\relax }{figure.caption.14}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Lernen in KNN}{15}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lol}{\addvspace {10\p@ }}
\newlabel{c:lernen_knn}{{3}{15}{Lernen in KNN}{chapter.3}{}}
\acronymused{KNN}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Lernen durch Modifikation der Gewichte}{15}{section.3.1}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Lernarten}{15}{section.3.2}}
\acronymused{KNN}
\citation{zell}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Lernverfahren}{16}{section.3.3}}
\acronymused{KNN}
\citation{hebb}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Lernregel}{17}{section.3.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Hebbsche Lernregel}{17}{subsection.3.4.1}}
\acronymused{KNN}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Delta-Regel}{18}{subsection.3.4.2}}
\newlabel{eq:delta-regel}{{3.3}{18}{Delta-Regel}{equation.3.4.3}{}}
\newlabel{eq:delta-wert}{{3.4}{18}{Delta-Regel}{equation.3.4.4}{}}
\acronymused{KNN}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}Backpropagation-Regel}{18}{subsection.3.4.3}}
\newlabel{ch:bb-regel}{{3.4.3}{18}{Backpropagation-Regel}{subsection.3.4.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Gradienentenabstiegsverfahren}{18}{section*.15}}
\newlabel{eq:Fehlerfunktion}{{3.5}{18}{Gradienentenabstiegsverfahren}{equation.3.4.5}{}}
\acronymused{KNN}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Bei zwei verschiedenen Gewichten kann die Fehlerfunktion E als Fl\"ache dargestellt werden\relax }}{19}{figure.caption.16}}
\newlabel{abb:Fehlerfl\"ache}{{3.1}{19}{Bei zwei verschiedenen Gewichten kann die Fehlerfunktion E als Fläche dargestellt werden\relax }{figure.caption.16}{}}
\acronymused{KNN}
\citation{rey_wender}
\newlabel{eq:delta-bruchteil}{{3.6}{20}{Gradienentenabstiegsverfahren}{equation.3.4.6}{}}
\newlabel{eq:Gesamtfehler}{{3.8}{20}{Gradienentenabstiegsverfahren}{equation.3.4.8}{}}
\newlabel{eq:Fehlermuster}{{3.9}{20}{Gradienentenabstiegsverfahren}{equation.3.4.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Der Backpropagation-Algorithmus}{21}{section*.17}}
\acronymused{KNN}
\newlabel{eq:deltaregel-erweitert}{{3.11}{22}{Der Backpropagation-Algorithmus}{equation.3.4.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Auswahl der Aktivierungsfunktion}{22}{section.3.5}}
\newlabel{c:aktivierungsfunktion_lernen}{{3.5}{22}{Auswahl der Aktivierungsfunktion}{section.3.5}{}}
\acronymused{KNN}
\acronymused{MLP}
\newacro{MLP}[\AC@hyperlink{MLP}{MLP}]{mehrlagige Perzeptron}
\newacro{KI}[\AC@hyperlink{KI}{KI}]{K\"unstliche Intelligenz}
\newacro{KNN}[\AC@hyperlink{KNN}{KNN}]{k\"unstliche neuronale Netze}
\newacro{GUI}[\AC@hyperlink{GUI}{GUI}]{Grafical User Interface}
\@writefile{toc}{\contentsline {chapter}{\nonumberline Abbildungsverzeichnis}{24}{chapter*.19}}
\acronymused{MLP}
\acronymused{MLP}
\@writefile{toc}{\contentsline {chapter}{\nonumberline Tabellenverzeichnis}{25}{chapter*.20}}
\bibcite{bidelman}{{1}{2010}{{Bidelman}}{{}}}
\bibcite{brij}{{2}{2015}{{Brij}}{{}}}
\bibcite{buckler}{{3}{2013}{{Buckler}}{{}}}
\bibcite{corves}{{4}{2005}{{Corves}}{{}}}
\bibcite{ertel}{{5}{2016}{{Ertel}}{{}}}
\bibcite{hebb}{{6}{1949}{{Hebb}}{{}}}
\bibcite{hillmann}{{7}{}{{Hillmann}}{{}}}
\bibcite{hoffmann}{{8}{1993}{{Hoffmann}}{{}}}
\bibcite{koenecke}{{9}{2016}{{Koenecke}}{{}}}
\bibcite{kramer}{{10}{2009}{{Kramer}}{{}}}
\bibcite{manhart}{{11}{2017}{{Manhart }}{{}}}
\bibcite{minsky_papert}{{12}{1969}{{Minsky \& Papert}}{{}}}
\@writefile{toc}{\contentsline {chapter}{\nonumberline Literaturverzeichnis}{26}{chapter*.21}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lol}{\addvspace {10\p@ }}
\bibcite{rey_wender}{{13}{1982}{{Rey \& Wender}}{{}}}
\bibcite{riley}{{14}{2017}{{Riley}}{{}}}
\bibcite{rimscha}{{15}{2014}{{Rimscha}}{{}}}
\bibcite{russell-norvig}{{16}{2010}{{Russell \& Norvig}}{{}}}
\bibcite{scherer}{{17}{1997}{{Scherer}}{{}}}
\bibcite{sethBling}{{18}{2015}{{SethBling}}{{}}}
\bibcite{turing}{{19}{1950}{{Turing}}{{}}}
\bibcite{wolf}{{20}{2009}{{Wolf}}{{}}}
\bibcite{wunderlich}{{21}{2017}{{Wunderlich-Pfeiffer}}{{}}}
\bibcite{zell}{{22}{1994}{{Zell}}{{}}}
