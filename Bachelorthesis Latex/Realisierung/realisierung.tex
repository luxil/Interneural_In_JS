% !TEX root = BachelorthesisNeu.tex
\input{../BachelorthesisMain/hawmt-abschlussarbeits-header}

\begin{document}
%______________________________________

\chapter{Das ursprüngliche Programm von Koenecke}

\section{Problemstellung und Netzwerktopologie}
Die Anwendung soll in der Lage sein, Klassifizierungsprobleme zu lösen, da diese zum Einstieg greifbarer sind als beispielsweise Regressionsprobleme. Zur Lösung dieser Problemstellung eignet es sich, als Topologie für das KNN das MLP festzusetzen. Bei jeder Initialisierung eines neuen MLP werden die Gewichtswerte zufällig vom Programm festgelegt. Jeder Datensatz besteht aus zwei Eingabewerten, denen eine Klasse zugeordnet wird, wobei es drei verschiedene Klassen gibt. Ziel des Trainings des KNN ist es, zu jeder Eingabemöglichkeit eine Klasse bestimmen zu können, indem ähnliche Eingabekombinationen gleich klassifiziert werden.

\section{Technologie}
%Das Programm von Koenecke lässt sich technisch in zwei Bereiche einteilen, dem Backend und dem Frontend. Zum einen handelt es sich beim Backend um einen Server, der die Daten des KNN erzeugt. Beim Frontend wird anhand einer grafischen Oberfläche Konfigurationsmöglichkeiten für das KNN und eine Visualisierung der Struktur und Ergebnisse des KNN geboten.

Mit der Programmiersprache  \emph{Google Go (golang)} hatte Koenecke eine ausführbare Anwendung \emph{mlpmain\_windows\_amd64.exe} erstellt, dessen Herzstück die Implementierung des KNN als Netzwerk-Bibliothek ist. Beim Ausführen der Anwendung wird ein Server zum Port 8080 gestartet. Der Server ist dafür zuständig, die komplexeren Berechnungen für das KNN im Hintergrund durchführen zu lassen. 

Die grafische Oberfläche, im Folgenden soll hierfür der Fachbegriff \ac{GUI} benutzt werden, wurde mit HTML, CSS und Javascript realisiert. Zur grafischen Darstellung von Daten wie die Topologie des KNN wurde die Javascript-Bibliothek D3.js verwendet, die extra darauf ausgelegt ist, Daten manipulieren und (interaktiv) visualisieren zu können. Die GUI bietet Konfigurationsmöglichkeiten und eine Visualisierung der Struktur und Ergebnisse des KNN.

Zur Kommunikation zwischen Backend und Frontend wurde ein Webserver als Schnittstelle mit \emph{gopkg.in/igm/sockjs-go.v2/sockjs}, ein Paket zur Websocket Emulation, realisiert. Unter der Adresse \url{http://localhost:8080} im Webbrowser wird die GUI der Anwendung für die Daten der Netzwerk-Bibliothek aufgerufen. 


\section{Festgelegte Parameter}
Um die Bedienung und die Übersichtlichkeit der GUI zu erleichtern, entschied sich Koenecke dafür, bestimmte Parameter festzulegen. Die Beschränkung auf zwei Eingabe- und drei Ausgabewerte ermöglicht eine zweidimensionale Visualisierung der Inhalte, eine dreidimensionale Visualisierung wäre deutlich aufwändiger gewesen. Zudem sind in seiner Anwendung als Aktivierungsfunktion eine sigmoide Funktion und als Lernregel die Backpropagation-Regel festgelegt, für die Lernrate wurde ebenfalls schon ein vordefinierter Wert gesetzt. 

\section{Aufbau der GUI}
\subsection{Netzwerktopologie}
Das erste Element der Anwendung (siehe Abb. \ref{abb:network-topologie}) dient zur Konfiguration der Topologie. Es ist möglich, bis zu 5 Hidden-Layer hinzuzufügen und sie auch wieder zu entfernen, zudem gibt es bei jedem Hidden-Layer die Möglichkeit, die Anzahl der Neuronen zwischen 1-9 festzulegen. Beim Drücken auf den Button \emph{apply} wird direkt ein MLP bzw. Netzwerk mit der gewünschten Topologie als Graph visualisiert und kann wie in Abb. \ref{abb:network-graph} aussehen. Die Dicke der Linien soll die Größe der Gewichtswerte andeuten, die Farbe der Linien, welches Vorzeichen die Gewichtswerte haben. 

\begin{figure}[htp]     % h=here, t=top, b=bottom, p=page
\centering
\includegraphics[width=0.25\textwidth]{../Realisierung/bilder/network-topologie} 
\caption{Konfiguration der Netzwerktopologie}\label{abb:network-topologie}
\end{figure}

\begin{figure}[htp]     % h=here, t=top, b=bottom, p=page
\centering
\begin{minipage}[b]{.45\textwidth}
	\centering
	
\includegraphics[width=1\textwidth]{../Realisierung/bilder/network-graph}
\captionsetup{format=plain}\caption{Darstellung eines Netzwerkgraphs (untrainiert)}\label{abb:network-graph}
\end{minipage}
\begin{minipage}[b]{.45\textwidth}
\centering
\includegraphics[width=1\textwidth]{../Realisierung/bilder/network-graph-t} 
\captionsetup{format=plain}\caption{Darstellung eines Netzwerkgraphs (trainiert)}\label{abb:network-graph-t}
\end{minipage}
\end{figure}

\subsection{Training}
\subsubsection{Erzeugung der Trainingsdaten}
Zur Erzeugung der Trainingsdaten gibt es eine Fläche von $300\times300$ px, die als Koordinatensystem gesehen werden kann, auf der rote, grüne und blaue Punkte eingetragen werden können (siehe Abb. \ref{abb:training}). Die Wahl auf diese Farben als Klassen fiel laut Koenecke aufgrund der dadurch gegebenen Möglichkeit, so optimal den RGB-Farbraum darstellen zu können. Jeder Punkt ist eine Darstellung eines Trainingsdatensatzes, bei dem die x- und die y-Koordinate die Eingabe und die Farbe die erwartete Ausgabe bzw. Klasse darstellt. 

\subsubsection{Trainieren des Netzwerks}
Sind die Trainingsdaten festgelegt, kann über den Button \emph{train network} das Training gestartet werden (siehe Abb. \ref{abb:trainingstart}). Der Slider ermöglicht es, das Training zu beschleunigen oder zu verlangsamen. Beim Trainingsprozess verändern sich die Gewichtswerte, dies ist auch bei der Visualisierung des Netzwerkgraphs zu sehen, bei dem sich die Dicken der Linien ändern (siehe Abb. \ref{abb:network-graph-t}).


\begin{figure}[htp]     % h=here, t=top, b=bottom, p=page
\centering
\begin{minipage}[b]{.45\textwidth}
	\centering
\includegraphics[width=0.4\textwidth]{../Realisierung/bilder/training}
\captionsetup{format=plain}\caption{Erzeugung der Trainingsdaten}\label{abb:training}
\end{minipage}
\begin{minipage}[b]{.45\textwidth}   
\centering
\includegraphics[width=0.6\textwidth]{../Realisierung/bilder/trainingstart} 
\captionsetup{format=plain}\caption{Start des Trainings mit 10-facher Geschwindigkeit}\label{abb:trainingstart}
\end{minipage}
\end{figure}

\subsection{Präsentation der Netzwerkergebnisse}
\subsubsection{Vorschau der Ausgabe}
Bei jedem Trainingsschritt wird für jeden Punkt des Eingabekoordinatensystems, d.h für jede mögliche Eingabemöglichkeit der x- und der y-Koordinate,\footnote{In diesem Fall gibt es also $300\cdot300=90000$ Eingabemöglichkeiten.} drei Ausgabewerte berechnet, die in der Anwendung die RGB-Werte repräsentieren. Dadurch ergibt sich die Möglichkeit, die Trainingsergebnisse als Bild zu visualisieren (siehe z.B. Abb. \ref{abb:output-preview1}). Jeder Ausgabewert steht dafür, wie hoch die Wahrscheinlichkeit ist, dass der Punkt zu einer bestimmten Klasse bzw. Farbe gehört. Dadurch kann sich für einen Punkt eine Mischfarbe ergeben, wenn sie zwei ähnlich hohe Ausgabewerte hat, meist befindet sie sich in dem Falle nahe der Hyperebene (siehe z.B. Abb. \ref{abb:output-preview2}, bei dem manche Punkte die Mischfarbe gelb-orange oder lila haben). Je mehr Trainingsschritte durchlaufen werden, desto eindeutiger kann jedem Punkt eine Farbe bzw. eine Klasse zugeordnet werden, was sich in der Vorschau dadurch bemerkbar macht, dass die Farben gesättigter und die Kanten schärfer sind (siehe z.B. Abb. \ref{abb:output-preview3}).

\begin{figure}[htp]     % h=here, t=top, b=bottom, p=page
\centering
\begin{minipage}[b]{.32\textwidth}
	\centering
	\includegraphics[width=0.9\textwidth]{../Realisierung/bilder/output-preview1} 
	\captionsetup{format=plain}\caption{Vorschau nach einer halben Million Beispieldaten}					\label{abb:output-preview1}
\end{minipage}
\begin{minipage}[b]{.32\textwidth}
	\centering
	\includegraphics[width=0.9\textwidth]{../Realisierung/bilder/output-preview2} 
	\captionsetup{format=plain}\caption{Vorschau nach einer Million Beispieldaten}	\label{abb:output-preview2}
\end{minipage}
\begin{minipage}[b]{.32\textwidth}
	\centering
	\includegraphics[width=0.9\textwidth]{../Realisierung/bilder/output-preview3} 
	\captionsetup{format=plain}\caption{Vorschau nach drei Millionen Beispieldaten}	\label{abb:output-preview3}
\end{minipage}
\end{figure}

\subsubsection{Netzwerkinfo}
Unter der Vorschau werden dem Anwender bestimmte Informationen des Netzes gezeigt (siehe Abb. \ref{abb:network-info}). Die Information \emph{samples total} gibt die Anzahl der bisher durchgeführten Trainingsschritte an, \emph{samples coverage} wie viel Prozent der Trainingspunkte durch das KNN korrekt klassifiziert wurden und \emph{mean weight change} wie hoch die durchschnittliche Gewichtsänderung ist. 

\begin{figure}[htp]     % h=here, t=top, b=bottom, p=page
\centering
\includegraphics[width=0.3\textwidth]{../Realisierung/bilder/network-info} 
\caption{Netzwerkinfo während des Trainings}\label{abb:network-info}
\end{figure}

\chapter{Eigene Implementierungen}
\section{Implementierung des KNN in Javascript}
\subsection{Wahl der passenden Javascript Bibliothek für das KNN}
Bei der Recherche stellte sich heraus, dass es bereits einige Javascript Bibliotheken gibt, mit denen KNN erstellt und lokal im Browser trainiert werden können. In die engere Auswahl fielen \emph{brain.js}, \emph{Mind}, \emph{Neataptic} und \emph{Synaptic}. Für jede Bibliothek wurde ein Testprogramm geschrieben, um sie besser miteinander vergleichen zu können, wobei jedes Programm dieselben Trainingsbedingungen hat. Grob zusammengefasst, besteht jeder Trainingsdatensatz aus zwei Punkten, denen eine Klasse zugeordnet wurde. Für das Training wird bei jedem eine bestimmte Topologie festgelegt und eine bestimmte Anzahl an Iterationen durchgeführt. Am Ende des Trainings sollte angegeben werden, wie die berechneten Ausgabewerte der Punkte aussehen und wie lange das Training gedauert hat. Die Testprogramme befinden sich in der CD im Ordner \emph{NNBibTests}.\footnote{Ebenfalls wurde aus Interesse zum Vergleich für die Bibliothek \emph{ConvNetJS} ein Testprogramm geschrieben, die bei der Berechnung deutlich am präzisesten und schnellsten war, allerdings werden dort keine MLP zur Berechnung genutzt, sondern sogenannte \emph{Convolutional Neural Networks}, eine besondere Form von Feedforward Netzen.} 

Folgende Punkte waren wichtig für die Wahl der Bibliothek gewesen: 
\begin{itemize}
\item die Möglichkeit, ein MLP erstellen zu können
\item die Handhabung mit der Bibliothek (z.B. wie aufwändig ist es, den Code zur Erstellung des MLP oder des Trainings zu schreiben)
\item die Geschwindigkeit der Berechnung 
\item die Effektivität des Trainings
\item die Dokumentation der Bibliothek
\item die Übersichtlichkeit und Struktur des Codes der Bibliothek
\item zu einem geringen Teil die Aktivität an der Bibliothek, also wie oft die Bibliothek aktualisiert wurde und wann die letzte Aktualisierung stattfand
\end{itemize}

In der Handhabung konnten brain.js und Mind ganz gut punkten, so ist es schon mit wenigen Zeilen Code möglich, ein KNN zu erstellen. Allerdings sind die Konfigurationsmöglichkeiten des KNN mit den beiden Bibliotheken eher beschränkt und beide bieten eine eher dürftige Dokumentation. Bei Neataptic handelt es sich um eine Bibliothek, die für bestimmte Teile des Codes die Synaptic Bibliothek verwendet hat. Von allen betrachteten Bibliotheken bietet sie auch mit einem Wiki aus 30 Seiten die beste Dokumentation und bietet gegenüber Synaptic Extras wie die Visualisierung der Topologie des KNN, mehr implementierte Aktivierungsfunktion etc. Allerdings handelt es sich um Extras, die für den eigentlichen Zweck des Programms nicht notwendig sind. Schlussendlich fiel die Wahl auf Synaptic, deren ausschlaggebendster Vorteil die Möglichkeit ist, das Training über Web Worker durchführen lassen zu können. Was Web Worker sind und weshalb sie einen Vorteil für die Anwendung bringen, soll im Kapitel \ref{ch:multiWW} erläutert werden.

\subsection{Performancevorteile durch Nutzung von Web Workern}\label{ch:multiWW}
\subsubsection{Problem der Synchronität und des Single-Threadings}
Allgemein kann Javascript als eine Programmiersprache betrachtet werden, die normalerweise \emph{synchron} und \emph{single-threaded} läuft. Single-threaded bedeutet, dass zur gleichen Zeit nur ein Prozess stattfinden kann \citep{brij}. Von synchroner Programmierung spricht man, wenn der Start von einem Prozess das ganze Programm solange zum Stoppen bringt, bis der Prozess zu Ende ausgeführt wurde. Das bedeutet also, dass bei Javascript die Prozesse sequenziell bzw. nacheinander abgearbeitet werden. 

Bezogen auf die Anwendung für die Bachelorarbeit stellt diese Eigenschaft ein ungünstiges Problem da. Für das Training werden komplexe Berechnungen durchgeführt, besonders bei der Backpropagation finden viele Berechnungsschritte statt. Das Training des MLP mit der Methode \emph{train()} der Synaptic Bibliothek führte dazu, dass die Bedienung der GUI während des Trainings sehr träge wirkte. Bis auf Interaktionen vom Anwender wie ein Mausklick auf einen Button reagiert wurde, vergingen teilweise mehrere Sekunden. 

\subsubsection{Multithreading mithilfe von Web Workern}
Es musste eine passende Technik gefunden werden, die Nebenläufigkeit bzw. Multithreading in Javascript zu ermöglichen, d.h. mehrere Prozesse gleichzeitig ausführen lassen zu können. Denn um dem Anwender eine reaktionsschnelle GUI bieten zu können, müssen die Berechnungen für das Trainings nebenläufig laufen. Es gibt verschiedene Wege die Nebenläufigkeit in Javascript nachzuahmen, wie die Verwendung von \emph{setTimeout()}, \emph{setInterval()}, \emph{XMLHttpRequest} oder  Ereignis-Handlern \citep{bidelman}. Allerdings findet bei diesen Techniken alles immer noch im selben Hauptthread\footnote{Bei einem Thread handelt es sich um eine Aktivität innerhalb eines Prozesses. Jeder Prozess besteht aus mindestens einem Thread, dem Hauptthread. Neben dem Hauptthread können im Prozess noch weitere Threads ausgeführt werden \citep{wolf}.} im Browser statt, so wechseln sich lediglich die Prozesse der Skriptausführungen mit den Aktivitäten der GUI ab. Web Worker sind eine Javascript API\footnote{Schnittstelle zur Anwendungsprogrammierung (engl.: application programming interface} für HTML5, mit der tatsächlich im Hintergrund Skripte über mehrere Threads ausgeführt werden können. 

Zum Testen der Geschwindigkeit gibt es für die Synaptic Bibliothek im Ordner \emph{synaptic-withoutWW} ein Testprogramm, welches das Training normal ohne einen Web Worker ausführt und im Ordner \emph{synaptic-withWW} ein Testprogramm, welches das Training mit einem Web Worker ausführt. Auf dem getesteten Rechner fiel beim Vergleichen der Zeiten auf, dass sogar mit dem Web Worker eine bessere Zeit erzielt wurde (ca. 750 ms gegenüber ca. 1000 ms ohne Web Worker). Bei der Implementierung des MLP mit Synaptic ist auch deutlich aufgefallen, dass durch die Nutzung eines Web Workers die GUI während des Trainings deutlich schneller auf die Interaktionen des Anwenders reagiert hat. 

\subsection{Trainieren eines MLP mit Synaptic}
In der Javascript Datei \emph{neural-network.js} ist der Codeteil zu finden, in dem das MLP erstellt und trainiert wird. Es soll hier in dieser Arbeit nicht der ganze Code erläutert werden, sondern nur kurz auf den interessanten Codeteil, der für das Training eingegangen werden (siehe Listing \ref{lis:jsTraining}). 
Bei \emph{trainAsync()} handelt es sich sich um eine Methode für das Perzeptron, das Training in einem Webworker stattfinden lassen zu können. Der Parameter \emph{rate} legt die Lernrate und \emph{iterations} legt die maximale Anzahl an Iterationen fest, diese wurden vom Anwender zuvor in der GUI festgelegt. Der \emph{error} gibt den Wert an, der unterschritten werden muss, bis das Training beendet werden kann. Durch \emph{cost} wird die Fehlerfunktion festgelegt, in diesem Fall wird der MSE genommen, der im Kapitel \ref{ch:bb-regel} erwähnt wurde. Mit \emph{myTrainer.trainAsync()} wird ein Promise-Objekt \emph{promiseTrain} erstellt, der bei erfolgreicher Operation mit \emph{promiseTrain.then()}, die Trainingsergebnisse an die Funktion \emph{updateAndSendMessageForApp} übergibt.\footnote{Das Promise-Objekt wird dazu verwendet, um asynchrone Berechnungen durchführen zu können. Genaueres zu der Technologie kann auf der Seite \url{https://developer.mozilla.org/de/docs/Web/JavaScript/Reference/Global_Objects/Promise} nachgelesen werden.} Dieser führt wiederum die Operationen durchführt, damit die Ergebnisse später in der GUI visualisiert werden können. 

\begin{lstlisting}[language=JavaScript,caption={Codeausschnitt des Trainings eines MLP mit Synaptic},label=lis:jsTraining,captionpos=b]
var promiseTrain = myTrainer.trainAsync(trainingSet, {
	rate: messageForApp.nnConfigInfo.learningRate,
	iterations: iterations,
	error: 0.000000000000000000000000000000000000000000000000000000000001,
	cost: Trainer.cost.CROSS_ENTROPY
});
promiseTrain.then(function (results) {
	returnObj = {
		"myPerceptron": myPerceptron,
		"samplesTrained": (samplesTrained += iterations),
		"trainingsSetLength": trainingSet.length
	}
	getUpdatedMessageForApp(JSON.stringify(returnObj));
});
\end{lstlisting}

\subsubsection{Aufbau eines Perzeptrons in Synaptic}
Vereinfacht sieht der Code eines Perzeptrons in Synaptic als JSON wie in Listing \ref{lis:jsonPerc} aus. Die Attribute \emph{\glqq input\grqq } und\emph{\glqq output\grqq} bekommen beide jeweils als Wert ein Layer-Objekt (siehe Listing \ref{lis:jsonLayer}) zugewiesen, sie stellen die Eingabeschicht und die Ausgabeschicht des MLP dar. Das Attribut \emph{\glqq hidden\grqq} bekommt als Wert ein Array zugewiesen, welches aus mehreren Layer-Objekten besteht, welches die versteckten Schichten darstellen.
\begin{lstlisting}[language=json, caption={JSON eines Perzeptron-Objekts in Synaptic},label=lis:jsonPerc,captionpos=b]
{"layers" = {
	"hidden":[...a\dots a...],			
	"input":{...a\dots a...},
	"ouput":{...a\dots a...}
}}
\end{lstlisting}

Das Listing der JSON vom Layer-Objekt ist stark vereinfacht, indem nur die Attribute und Werte angezeigt werden, die für die Visualisierung des MLP in der GUI von Bedeutung sind.  

\begin{lstlisting}[language=json, caption={JSON eines Layer-Objekts},label=lis:jsonLayer,captionpos=b]
{"Layer":{		
	"list":[
		"Neuron":{
			"ID": 5,
			"activation": 0,
			"bias": 0.46,
			"connections":{
				"projected":{
					"Connection":{
						"from":{
							"ID": 5,
							...a\dots a...
						},
						"to":{
							"ID": 7,
							...a\dots a...
						},
						"weight": 6.553,
						...a\dots a...
					},
					...a\dots a...
				},
				...a\dots a...
			},
			...a\dots a...
		},
		...a\dots a...
	],
	"size": 5,
	...a\dots a...
}}
\end{lstlisting}

\subsection{Schnittstelle zwischen der Netzwerkbibliothek und der GUI}
Dem Vorteil von Web Workern, dass rechenintensive Skripte nicht mehr im Hauptthread ausgeführt werden müssen, stehen einige Nachteile gegenüber. Im Bezug auf die Anwendung für die Bachelorarbeit sind die wichtigsten Nachteile, dass Web Worker nicht in der Lage sind, das DOM\footnote{DOM: Document Object Model; Programmierschnittstelle für HTML und XML Dokumente} zu ändern und dass sie nicht auf globale Variablen und Funktionen zugreifen können \citep{buckler}. Für die Anwendung bedeutet dies, dass während des Trainingsvorgangs, indem die intensiven Berechnungen stattfinden, die GUI nicht verändert werden kann. 

In seiner Bachelorthesis hat Koenecke die Vermutung aufgestellt, dass eine Implementierung des KNN in Javascript wohl keine Schnittstelle zur Kommunikation zwischen der Netzwerkbibliothek und der GUI erfordert hätte \citep[S.42]{koenecke}. Im Grunde genommen hat er damit Recht, dies setzt jedoch voraus, dass die Berechnungen des Trainings im Hauptthread stattfinden, um währenddessen gleichzeitig die GUI verändern zu können. Wie bereits vorher erläutert, würde dies aber zu sehr die Performance verschlechtern und daher ist die Verwendung von Web Workern zu bevorzugen. Um einen Austausch der Trainingsergebnisse mit den Interaktionen der GUI zu ermöglichen, hat es sich also weiterhin angeboten die Schnittstelle zur Kommunikation beizubehalten. Zudem gewährleistet eine Trennung der Netzwerkbibliothek von der GUI eine bessere Übersichtlichkeit vom Code der Anwendung. Wie auch in Koeneckes Anwendung stellt die Javascript-Datei \emph{app.js} die Schnittstelle dar, mit dem Unterschied, dass in dieser Anwendung kein Server und daher keine emulierten Websockets mehr benötigt werden. 

\subsubsection{Aufbau eines message-Objektes für die GUI} 
In der Schnittstelle wird zur Erstellung des MLP mit der Methode \emph{newNetwork()} und zum Updaten des MLP mit der Methode \emph{updateNetwork()} in der GUI ein message-Objekt mit einer bestimmten Struktur benötigt (siehe Listing \ref{lis:jsonMessage}). Sie entspricht zum größten Teil dem message-Objekt von Koenecke, aufgrund der Erweiterungen in der Anwendung wurde das message-Objekt etwas erweitert. In der \emph{neural-network.js} wurden zur Erstellung der message-Objekte in diesem Format die Funktionen \emph{getMessageForApp()} und \emph{getUpdatedMessageForApp()} geschrieben.

\begin{lstlisting}[language=json, caption={JSON eines Perzeptron-Objekts in Synaptic},label=lis:jsonMessage,captionpos=b]
{"message" = {
	"bMaxIterationsReached": false,
	"nnConfigInfo": {
		"activationFunction": "relu",
		"learningRate": 0.01,
		"maxIterations": 100000
	},
	"id": 1,
	"graph":{
		"layers": [
			{
				"numberOfNeurons": 3,
				"weights":{
					"data":[
						{0.54, 0.945, 0.435},
						...a\dots a...
					]
				}
			},
			...a\dots a...
		],
		"sampleCoverage": 0,
		"samplesTrained":0,
		"weightChange":0
	}	
	"output":{
		"data": [
			[253, 0, 0],
			...a\dots a...
		]
	}
}}
\end{lstlisting}

\section{Erweiterungen an der GUI}
\subsection{Konfiguration der Aktivierungsfunktion und der Lernrate}
Die Konfiguration des MLP wurde erweitert, dass nicht nur die Topologie, sondern auch die Aktivierungsfunktion und die Lernrate festgelegt werden kann (siehe Abb. \ref{abb:network-config}). So kann der Anwender besser verstehen, inwiefern diese beiden Parameter das Training des Netzwerks beeinflussen können. Beide Parameter müssen beim Start festgelegt werden und können nicht während des Trainings verändert werden. 
\begin{figure}[htp]     % h=here, t=top, b=bottom, p=page
\centering
\includegraphics[width=0.25\textwidth]{../Realisierung/bilderErw/network-config} 
\caption{Die Aktivierungsfunktion und die Lernrate können nun verändert werden}\label{abb:network-config}
\end{figure}
\subsection{Erweiterungen beim Training}
Das Feld zum Setzen der Trainingspunkte wurde um zwei Koordinatenachsen erweitert, wodurch die Trainingspunkte besser positioniert werden können (siehe Abb. \ref{abb:training-sCreate}). Zudem kann der Anwender durch die Achsen möglicherweise besser verstehen, dass die x- und y-Koordinate als Eingabeparameter für jeden Trainingsdatensatz zu sehen sind. Ist ein Trainingspunkt ausgewählt, so hat man die Möglichkeit durch die beiden Eingabefelder \emph{x} und \emph{y} genaue Werte für die Koordinaten zu setzen oder den Trainingspunkt zu löschen. Die Liste darunter bietet eine weitere Übersicht, um zu veranschaulichen, welche Trainingspunkte beim Start des Trainings übergeben werden.
\begin{figure}[htp]     % h=here, t=top, b=bottom, p=page
\centering
\includegraphics[width=0.45\textwidth]{../Realisierung/bilderErw/trainingGes} 
\caption{Das neue Trainingselement}\label{abb:training-sCreate}
\end{figure}
Beim Eingabefeld \emph{max. Iterations} kann festgelegt werden, nach wie vielen Trainingsiterationen das Training gestoppt werden kann (siehe Abb. \ref{abb:training-sCreate}). Es ist möglich, nach dem Stoppen des Trainings den Wert zu erhöhen und das Training fortzuführen. Auch während des Trainings kann der Wert verändert werden. Gedacht ist die Erweiterung als eine Möglichkeit, besser die Effektivität des Trainings vergleichen zu können, wenn beispielsweise eine andere Aktivierungsfunktion gewählt wurde. Durch einen Klick auf den Button \emph{save samples} können die Trainingsdaten lokal im Webbrowser gespeichert werden, sodass man die Anwendung auch schließen kann und mit \emph{load samples} die Trainingsdaten zu einem späteren Zeitpunkt wieder aufrufen kann. 

\subsection{Aufgaben auswählen}
Der Anwender hat nun zusätzlich die Möglichkeit anhand gestellter Übungen spielerisch zu erlernen, wie das KNN für bestimmte Probleme konfiguriert werden sollte (siehe Abb. \ref{abb:exercise}). Zu jeder Übung gibt es eine \emph{tasklist} mit mehreren \emph{tasks}. Wurde eine Task gelöst, so ändert sich die Textfarbe der Task (siehe Abb. \ref{abb:tasksolved}). Momentan sind in der Anwendung lediglich \textcolor{red}{drei} Übungen vorhanden, die noch eher einfach gehalten sind. 

\begin{figure}[htp]     % h=here, t=top, b=bottom, p=page
\centering
\includegraphics[width=1\textwidth]{../Realisierung/bilderErw/exercise} 
\caption{Screenshot von einer Übung}\label{abb:exercise}
\end{figure}

\begin{figure}[htp]     % h=here, t=top, b=bottom, p=page
\centering
\includegraphics{../Realisierung/bilderErw/tasksolved} 
\caption{Der Task für die ReLu Aktivierungsfunktion wurde gelöst.}\label{abb:tasksolved}
\end{figure}

\chapter{Konfiguration eines KNN passend zur Problemstellung}


%______________________________________

\input{../Literaturliste/Literaturliste} 
\end{document}